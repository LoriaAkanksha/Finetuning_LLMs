{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcSNonIZZh9S",
    "outputId": "5e53cfa1-6967-47ac-dff4-c9d87a996b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradientai\n",
      "  Downloading gradientai-1.11.0-py3-none-any.whl (375 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.5/375.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aenum>=3.1.11 (from gradientai)\n",
      "  Downloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.5 (from gradientai)\n",
      "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.5->gradientai) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai) (1.16.0)\n",
      "Installing collected packages: aenum, pydantic, gradientai\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.1\n",
      "    Uninstalling pydantic-2.7.1:\n",
      "      Successfully uninstalled pydantic-2.7.1\n",
      "Successfully installed aenum-3.1.15 gradientai-1.11.0 pydantic-1.10.15\n"
     ]
    }
   ],
   "source": [
    "!pip install gradientai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQIHCo9aZod-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GRADIENT_WORKSPACE_ID\"]=\"gradient_workspace_id\"\n",
    "os.environ[\"GRADIENT_ACCESS_TOKEN\"]=\"gradient_access_token\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ij3i3ixIeOlm"
   },
   "outputs": [],
   "source": [
    "from gradientai import Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIBkRRVexMnc",
    "outputId": "4196b017-f9ca-4828-f9fc-2b6b03569abd"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model adapter with id 63a8d4c9-dd31-4e98-9c74-87e37084b38d_model_adapter\n",
      "Asking:  ### Instructions: what is the 'attention is all you need'? \n",
      "\n",
      " ### Response: \n",
      "Generated(before finetuning):  \"Attention is All You Need\" is a phrase that refers to the idea that with enough attention, a neural network can learn to perform any task. This concept is a central idea in the field of deep learning, particularly in the development of neural networks that use attention mechanisms to process and analyze data. The phrase is often associated with the work of researchers like Geoffrey Hinton, who has been a pioneer in the development of deep learning and attention-based neural networks.\n",
      "finetuning the model with iterations1\n",
      "finetuning the model with iterations2\n",
      "finetuning the model with iterations3\n",
      "Generated(after finetuning):  The \"Attention Is All You Need\" is a revolutionary architecture for sequence-to-sequence tasks, introduced by Ashish Vaswani et al. in the paper \"Attention Is All You Need.\" This model replaces traditional sequence-to-sequence models with a self-attention mechanism, which allows the model to weigh the importance of different parts of the input sequence when making predictions. \n",
      "\n",
      "The self-attention mechanism works by computing a weighted sum of the\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    gradient = Gradient()\n",
    "\n",
    "    base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\")\n",
    "\n",
    "    new_model_adapter = base_model.create_model_adapter(\n",
    "        name=\"my_test_model\"\n",
    "    )\n",
    "    print(f\"Created model adapter with id {new_model_adapter.id}\")\n",
    "\n",
    "    sample_query = \" ### Instructions: what is the 'attention is all you need'? \\n\\n ### Response: \"\n",
    "    print(f\"Asking: {sample_query}\")\n",
    "\n",
    "    ## Before Finetuning\n",
    "    completion = new_model_adapter.complete(query=sample_query, max_generated_token_count=100).generated_output\n",
    "    print(f\"Generated(before finetuning): {completion}\")\n",
    "\n",
    "    samples = [\n",
    "        {\"inputs\": \" ### Instructions: what is the 'attention is all you need'? \\n\\n ### Response:The “Attention Is All You Need” is a revolutionary architecture that replaces traditional sequence-to-sequence models with a self-attention mechanism for machine translation tasks. \"},\n",
    "        {\"inputs\": \" ### Instructions: what is the 'attention is all you need'? \\n\\n ### Response:“Attention Is All You Need” is a research paper by Ashish Vaswani et al. that proposes a new neural network architecture for sequence-to-sequence tasks, called the Transformer model.\"},\n",
    "        {\"inputs\": \" ### Instructions: what is the 'attention is all you need'? \\n\\n ### Response:'Attention Is All You Need' is a landmark 2017 research paper authored by eight scientists working at Google, responsible for expanding 2014 attention mechanisms proposed by Bahdanau et al.\"},\n",
    "        {\"inputs\": \" ### Instructions: what is the 'attention is all you need'? \\n\\n ### Response: Eight names are listed as authors on “Attention Is All You Need,” a scientific paper written in the spring of 2017.\"}\n",
    "    ]\n",
    "\n",
    "    ## Parameters for finetuning\n",
    "    num_epochs=3\n",
    "    count=0\n",
    "    while count<num_epochs:\n",
    "      print(f\"finetuning the model with iterations{ count+1 }\")\n",
    "      new_model_adapter.fine_tune(samples=samples)\n",
    "      count=count+1\n",
    "\n",
    "    ## After Finetuning\n",
    "    completion = new_model_adapter.complete(query=sample_query, max_generated_token_count=100).generated_output\n",
    "    print(f\"Generated(after finetuning): {completion}\")\n",
    "\n",
    "    new_model_adapter.delete()\n",
    "    gradient.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDpF0vO5Hng3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ag4YA7_Sw1qM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
